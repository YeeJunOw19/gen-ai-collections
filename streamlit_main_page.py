
import streamlit as st
from src.agentic.agentic_workflow import python_qa_quality

# Setup page configuration
st.set_page_config(page_title="Python Code Evaluation", page_icon=":robot_face:", layout="wide")

# Setup run configuration for LangGraph
RECURSION_LIMIT = 50


# --------------------------------------------------------------------------------------------------------------------
# Application Main Page
# --------------------------------------------------------------------------------------------------------------------

# Title of this main page
st.write("### Evaluating Python Code Generation of Local LLMs :robot_face:")

# Add markdown to describe the page and what is the purpose of this page
st.markdown(
    """
    With the advancement in Large Language Models (LLMs), the ability to generate code is increasingly important. The
    primary concern regarding code generation using cloud LLMs from major providers is the sharing of sensitive data and
    information with third parties that may be untrusted. This lead to an explosion of many pre-trained LLMs that can be
    downloaded locally and used for code generation.

    This in turns sprout a different problem, where the LLMs are used to generate code that is not as high quality. This
    lead to the need for fine-tuning these open-source LLMs to improve their performance. This application aims to compare
    two open-source model: LLama Instruct (base) and SmolLM (fine-tuned), and evaluate the performance of both of these
    models. The code generated from these two models may not be production ready or up to standard, but this serves as a
    baseline for evaluating whether to continue the future work of fine-tuning these models.

    ---
    """
)

# Starting point of the application
st.markdown(
    """
    To start, provide a question that is Python related that you would like to ask the LLMs to solve.
    """
)
input_question = st.text_input("Your Question:")
st.markdown("---")


# --------------------------------------------------------------------------------------------------------------------
# Generate Responses from the two LLM Models
# --------------------------------------------------------------------------------------------------------------------

# Create and compile a LangGraph
if input_question:
    lang_app = python_qa_quality.agents_workflow().compile()

    # First get the output from Llama model
    llama_output = lang_app.invoke(
        {"question": input_question, "competitor_name": "Competitor1"},
        config={"recursion_limit": RECURSION_LIMIT}
    )

    # Get the output from the SmolLM model
    smol_output = lang_app.invoke(
        {"question": input_question, "competitor_name": "Competitor2"},
        config={"recursion_limit": RECURSION_LIMIT}
    )


    # ----------------------------------------------------------------------------------------------------------------
    # Output the final Python code of both models into different sub-sections
    # ----------------------------------------------------------------------------------------------------------------

    st.write("#### Generated Python Code")
    st.markdown(
        """
        Below shows the Python code generated by both models.
        """
    )

    # Create two columns to hold two of the python output
    col1, col2 = st.columns(2)

    with col1:
        st.header("Llama Instruct Base Model")
        st.code(llama_output["answer"], language="python")

    with col2:
        st.header("SmolLM Python Fine-tuned Model")
        st.code(smol_output["answer"], language="python")

    st.markdown("---")


    # ----------------------------------------------------------------------------------------------------------------
    # Provide evaluation from ChatGPT
    # ----------------------------------------------------------------------------------------------------------------

    st.write("#### Python Code Quality Evaluation")
    st.markdown(
        """
        To evaluate the quality of code generated by the two models, we provide both sets of code OpenAI and have OpenAI
        evaluate the quality of both sets of code. The evaluation is provided below.
        """
    )

    col1, col2 = st.columns(2)

    with col1:
        st.header("Llama Instruct Base Model")
        st.code(llama_output["quality"], language="markdown")

    with col2:
        st.header("SmolLM Python Fine-tuned Model")
        st.code(smol_output["quality"], language="markdown")
